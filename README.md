## ðŸ“š Research Notes Repository

This repository serves as a personal collection of notes, reviews, and reflections on key papers and ideas that Iâ€™ve found useful or worth remembering. Itâ€™s not intended for public release or wide consumptionâ€”but Iâ€™ve left it public in case it happens to help someone else, too.

Use at your own discretion, and feel free to explore!

Timeline Recap
2017 â†’ Attention Is All You Need (Transformer)
2018 (early) â†’ ELMo
2018 (mid) â†’ GPT
2018 (late) â†’ BERT
2019 â†’ GPT-2
2020â€“2021 â†’ MoE revival (GShard, Switch Transformer)


[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.md](https://github.com/dyh2111/paper-reviews-resouces/blob/7bd1595e124f1b4ea04a2dba7d6a241b165e75e4/BERT%3A%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.md)
